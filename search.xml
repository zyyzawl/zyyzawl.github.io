<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Hello World</title>
      <link href="/2024/04/23/di-yi-tian-xue-xi/"/>
      <url>/2024/04/23/di-yi-tian-xue-xi/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://allie.github.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">文章摘要:</span><br><span class="line">本文主要介绍了机器学习的一些基础内容。</span><br><span class="line">&lt;!--more--&gt;</span><br><span class="line">正文：</span><br><span class="line">机器学习简介</span><br><span class="line">1.机器学习背景</span><br><span class="line">伴随着计算机计算能力的不断提升以及大数据时代的迅发展人工智能也取得了前所未有的进步。</span><br><span class="line"></span><br><span class="line">很多企业均开始使用机器学习的相关技术于大部分行业中，以此获得更为强大的洞察力，也为企业的日常生活和企业运营带来了很大的帮助，从而提高了整个产品的服务质量。</span><br><span class="line"></span><br><span class="line">机器学习的典型应用领域有：搜索引擎、自动驾驶、量化投资、计算机视觉、信用卡欺诈检测、游戏、数据挖掘、电子商务、图像识别、自然语言处理、医学诊断、证券金融市场分析以及机器人等相关领域，故在一定程度上，机器学习相关技术的进步也提升了人工智能领域发展的速度。</span><br><span class="line"></span><br><span class="line">2.机器学习简介</span><br><span class="line">机器学习(MachineLearning)，作为计算机科学的子领域，是人工智能领域的重要分支和实现方式。</span><br><span class="line"></span><br><span class="line">机器学习的思想：计算机程序随着经验的积累，能够实现性能的提高。对于某一类任务T及其性能度量P，若一个计算机程序在T上以P衡量的性能随着经验E而自我完善，那么就称这个计算机程序在从经验E学习。</span><br><span class="line"></span><br><span class="line">主要的基础理论：数理统计，数学分析，概率论，线性代数，优化理论，数值逼近、计算复杂性理论。</span><br><span class="line"></span><br><span class="line">机器学习的核心元素：算法、数据以及模型。</span><br><span class="line"></span><br><span class="line">3.机器学习简史</span><br><span class="line">作为一门不断发展的学科，机器学习尽管在最近几年才发展成为一门独立的学科。</span><br><span class="line"></span><br><span class="line">起源于20世纪50年代以来人工智能的逻辑推理、启发式搜索、专家系统、符号演算、自动机模型、模糊数学以及神经网络的反向传播BP算法等。如今作为机器学习重要的基础理论。</span><br><span class="line">在1950年代，已经有了机器学习的相关研究。代表工作主要是F.Rosenblatt基于神经感觉科学提出的计算机神经网络，即感知器。随后十年，用于浅层学习的神经网络风靡一时，尤其是MarvinMinsky提出了著名的XOR问题和感知器线性度不可分割的问题。</span><br><span class="line"></span><br><span class="line">局限：由于计算机的计算能力有限，因此很难训练多层网络。通常使用仅具有一个隐藏层的浅层模型。尽管已经陆续提出了各种浅层机器学习模型，但理论分析和应用方面都已产生。但是，理论分析和训练方法的难度要求大量的经验和技能。而随着最近邻算法和其他算法的相继提出，在模型理解，准确性和模型训练方面已经超越了浅层模型。机器学习的发展几乎停滞不前。</span><br><span class="line"></span><br><span class="line">在2006年，希尔顿（Hinton）发表了一篇关于深度信念网络的论文，Bengio等人发表了关于“深度网络的贪婪分层明智训练”的论文，而LeCun团队发表了基于能量模型的“稀疏表示的有效学习”。</span><br><span class="line"></span><br><span class="line">这些事件标志着人工智能正式进入深度网络的实践阶段。同时，云计算和GPU并行计算为深度学习的发展提供了基本保证，尤其是近年来，机器学习它在各个领域都实现了快速发展。新的机器学习算法面临的主要问题更加复杂。机器学习的应用领域已从广度发展到深度，这对模型的训练和应用提出了更高的要求。</span><br><span class="line"></span><br><span class="line">随着人工智能的发展，冯·诺依曼有限状态机的理论基础变得越来越难以满足当前神经网络中层数的要求。这些都给机器学习带来了挑战。</span><br><span class="line">2.什么是机器学习？</span><br><span class="line">监督学习 supervised learning;</span><br><span class="line">非监督学习 unsupervised learning;</span><br><span class="line">半监督学习 semi-supervised learning;</span><br><span class="line">强化学习 reinforcement learning;</span><br><span class="line">监督学习是不断向计算机提供数据（特征），并告诉计算机对应的值（标签），最后通过大量的数据，让计算机自己学会判断和识别。例如Google Photo，你在APP中输入狗，这时就会弹出与狗相关的图片，这背后使用的算法就是监督学习。通过告诉计算机狗长什么样（特征），最后教会计算机认识狗（标签）。再比如今日头条，你使用APP的时间越长，给你推送的内容就越是你平时感兴趣的内容。通过对用户平时日常使用数据（特征）的分析，找到用户的兴趣爱好（标签），进而更精准的推送内容。</span><br><span class="line"></span><br><span class="line">非监督学习与监督学习的区别是，只向计算机提供数据（特征），但并不提供对应的值（标签）。例如需要计算机学会识别猫和狗，这时仅提供猫和狗的图片（特征），但是并不告诉计算机，哪些图片是猫，哪些图片是狗，让计算机自己去总结归纳出两者的特征规律。</span><br><span class="line"></span><br><span class="line">半监督学习是综合了监督学习和非监督学习两者的特点，利用少量有标签的样本，和大量没有标签的样本对计算机进行训练。</span><br><span class="line"></span><br><span class="line">强化学习是将计算机放入一个陌生的环境中，让它自己去学习，其中包含了4个关键要素，分别是环境（environment）、状态（state）、行动（action）和奖励（reward）。例如要设计一款自动投篮机器，首先让机器自己去选择投篮的角度、力度等动作进行尝试，告诉机器如果投篮命中便能获得奖励，之后机器会根据练习所产生的数据，不断修改自身的动作策略，经过数次迭代之后，学习并完成投篮任务。战胜李世石的AlphaGo，所使用的就是强化学习。强化学习与监督学习和非监督学习最大的不同是，不需要使用海量的数据去“喂养”机器，而是通过不断地尝试去获取数据。</span><br><span class="line"></span><br><span class="line">使用Python进行机器学习时，都会用到一个非常强大的第三方包，那就是scikit-learn。</span><br><span class="line"></span><br><span class="line">Sklearn包含了四类算法，分别是回归（regression）、分类（classification）、聚类（clustering）和降维（dimensionality reduction）。其中回归和分类是监督式学习，下面使用Python对简单线性回归和逻辑回归分类进行简要介绍。</span><br><span class="line">以上是我今天想要介绍的东西，谢谢大家欣赏！</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>机器学习——KNN算法</title>
      <link href="/2024/04/23/da-qia-di-er-tian-zuo-ye/"/>
      <url>/2024/04/23/da-qia-di-er-tian-zuo-ye/</url>
      
        <content type="html"><![CDATA[<p>​     机器学习笔记</p><p><img src="C:\Users\23792\AppData\Roaming\Typora\typora-user-images\image-20240423223130913.png" alt="image-20240423223130913"></p><p>from sklearn.neighbors import KNeighborsClassifier  #导入所需要用到的包<br>#创建模型<br>x &#x3D; [[0],[1],[2],[3]]<br>y &#x3D; [0,0,1,1]<br>model &#x3D; KNeighborsClassifier(n_neighbors&#x3D;3)<br>model.fit(x,y)<br>mypre &#x3D; model.predict([[4]])<br>#输出预测值<br>print(f’预测值:{mypre}’)</p><p>一、KNN原理基础<br>KNN的算法原理，可以简单如下描述：<br>一个数据集中存在多个已有标签的样本值，这些样本值共有的n个特征构成了一个多维空间N。当有一个需要预测&#x2F;分类的样本x出现，我们把这个x放到多维空间n中，找到离其距离最近的k个样本，并将这些样本称为最近邻（nearest neighbour）。对这k个最近邻，查看他们的标签都属于何种类别，根据”少数服从多数，一点算一票”的原则进行判断，数量最多标签类别就是x的标签类别。其中涉及到的原理是“越相近越相似”，这也是KNN的基本假设。 KNN中的K代表的是距离需要分类的测试点x最近的K个样本点，如果不输入这个值，KNN算法中的重要部分“选出K个最近邻”就无法实现。</p><p>若数据集只有两个特征，则针对于数据集的描述可用二维平面空间图来表示。如下图，二位平面空间的横坐标是特征1，纵坐标是特征2，每个样本点的分类（正或负）是该组样本的标签。图中给出了位于平面中心的，需要分类的数据点x，并用绿色分别标注了k为1，2，3时的最近邻状况。在图a中，x的1-最近邻是一个负例，因此x被指派到负类。图c中，3-最近邻中包括两个正例和一个负例，根据“少数服从多数原则”，点x被指派到正类。在最近邻中正例和负例个数相同的情况下（图b），算法将随机选择一个类标号来分类该点。</p><h2 id="二、sklearn的基本建模流程"><a href="#二、sklearn的基本建模流程" class="headerlink" title="二、sklearn的基本建模流程"></a>二、sklearn的基本建模流程</h2><p>#&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;注意该代码仅作展示，无法运行&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;#</p><p>from sklearn.neighbors import KNeighborsClassifier #导入需要的模块</p><p>clf &#x3D; KNeighborsClassifier(n_neighbors&#x3D;k)          #实例化<br>clf &#x3D; clf.fit(X_train,y_train)                     #用训练集数据训练模型<br>result &#x3D; clf.score(X_test,y_test)                  #导入测试集，从接口中调用需要的信息</p><p>三、KNN算法调优：选取最优的K值<br>从KNN的原理中可见，是否能够确认合适的k值对算法有极大的影响。如果K太小，则最近邻分类器容易受到由于训练数据中的噪声而产生的过分拟合的影响；相反。如果k太大，最近邻分类器可能会将测试样例分类错误，因为k个最近邻中可能包含了距离较远的，并非同类的数据点（如下图，k近邻由绿色表示）。因此，超参数K的选定是KNN中的头号问题。</p><p>那我们怎样选择一个最佳的K呢？在这里我们要使用机器学习中的神器：参数学习曲线。参数学习曲线是一条以不同的参数取值为横坐标，不同参数取值下的模型结果为纵坐标的曲线，我们往往选择模型表现最佳点的参数取值作为这个参数的取值。</p><p>以手写数据集为例：</p><blockquote><p>代码如下（示例）：</p><p>from sklearn.neighbors import KNeighborsClassifier<br>from sklearn.datasets import load_digits<br>from sklearn.model_selection import train_test_split<br>import matplotlib.pyplot as plt<br>#探索数据集<br>data &#x3D; load_digits()</p><p>Xtrain,Xtest,Ytrain,Ytest &#x3D; train_test_split(X,y              #特征和标签<br>                                           ,test_size&#x3D;0.3    #测试集所占的比例<br>                                           ,random_state&#x3D;1)</p><h1 id="绘制学习曲线"><a href="#绘制学习曲线" class="headerlink" title="绘制学习曲线"></a>绘制学习曲线</h1><p>score &#x3D; [] #用来存放模型预测结果<br>krange &#x3D; range(1,20) </p><p>for i in krange:<br>   clf &#x3D; KNeighborsClassifier(n_neighbors&#x3D;i)<br>   clf &#x3D; clf.fit(Xtrain,Ytrain)<br>   score.append(clf.score(Xtest,Ytest))<br>score<br>bestK &#x3D; krange[score.index(max(score))]<br>print(bestK)<br>print(max(score))</p><p>plt.rcParams[‘font.family’] &#x3D; [‘sans-serif’]<br>plt.rcParams[‘font.sans-serif’] &#x3D; [‘SimHei’’]</p><p>plt.figure(figsize&#x3D;(6,4),dpi&#x3D;80)  #dpi是像素值<br>plt.plot(krange,score)</p><p>plt.title(‘K值学习曲线’,fontsize&#x3D;15)<br>plt.xlabel(‘K值’,fontsize&#x3D;12)<br>plt.ylabel(‘预测准确率’,fontsize&#x3D;12)<br>plt.xticks(krange[::3])  # x轴刻度<br>plt.show()</p></blockquote><p>四、KNN中距离的相关讨论</p><ol><li><p>KNN使用的是什么距离？<br>KNN属于距离类模型，原因在于它的样本之间的远近是靠数据距离来衡量的。欧几里得距离、曼哈顿距离、切比雪夫距离、闵可夫斯基距离等都是很常见的距离衡量方法。KNN中默认使用的是欧氏距离（也就是欧几里得距离）。</p></li><li><p>距离类模型的归一化需求<br>我们再看一下，欧氏距离的计算公式： d ( A , B ) &#x3D; ∑ i &#x3D; 1 n ( x i A − x i B ) 2 d(A, B) &#x3D; \sqrt{\sum_{i &#x3D; 1}^{n}(x_{iA}-x_{iB})^2}d(A,B)&#x3D;<br>∑<br>i&#x3D;1<br>n</p></li></ol><p> (x<br>iA</p><p> −x<br>iB</p><p> )<br>2</p><p>试想看看，如果某个特征 𝑥𝑖 的取值非常大，其他特征的取值和它比起来都不算什么，那距离的大小很大程度上都会由这个巨大特征 𝑥𝑖 来决定，其他的特征之间的距离可能就无法对 𝑑(𝐴,𝐵) 的大小产生什么影响了，这种现象会让KNN这样的距离类模型的效果大打折扣。<br>有的特征数值很大，有的特征数值很小，这种现象在机器学习中被称为“量纲不统一”.</p><p>在实际分析情景当中，绝大多数数据集都会存在各特征值量纲不同的情况，此时若要使用KNN分类器，则需要先对数据集进行归一化处理，即是将所有的数据压缩到同一个范围内。这样可以避免模型偏向于数值很大但本身可能不是那么重要的一些特征。</p><p>思考：先切分训练集和测试集，还是先进行归一化？<br><strong>真正正确的方式是，先分训练集和测试集，再归一化！</strong>为什么呢？想想看归一化的处理手段，我们是使用数据中的最小值和极差在对数据进行压缩处理，如果我们在全数据集上进行归一化，那最小值和极差的选取是会参考测试集中的数据的状况的。因此，当我们归一化之后，无论我们如何分割数据，都会由一部分测试集的信息被“泄露”给了训练集（当然，也有部分训练集的信息被泄露给了测试集，但我们不关心这个），这会使得我们的模型效果被高估。</p><p>#导入所需各种模块和包<br>from sklearn.neighbors import KNeighborsClassifier<br>from sklearn.model_selection import train_test_split<br>from sklearn.preprocessing import MinMaxScaler as mms<br>from sklearn.datasets import load_wine</p><p>#导入数据集并提取特征和标签<br>data &#x3D; load_wine()<br>X &#x3D; data.data<br>y &#x3D; data.target</p><p>#切分训练集和测试集<br>Xtrain,Xtest,Ytrain,Ytest &#x3D; train_test_split(X,y,test_size&#x3D;0.3,random_state&#x3D;0)<br>#数据归一化<br>MMS &#x3D; mms().fit(Xtrain)              #这一步是在学习训练集，生成训练集上的极小值和极差<br>Xtrain_mms &#x3D; MMS.transform(Xtrain)   #用训练集上的极小值和极差归一化训练集<br>Xtest_mms &#x3D; MMS.transform(Xtest)     #用训练集上的极小值和极差归一化测试集<br>#建模并评估结果<br>clf &#x3D; KNeighborsClassifier()<br>clf &#x3D; clf.fit(Xtrain_mms,Ytrain)<br>score &#x3D; clf.score(Xtest_mms,Ytest)<br>score<br>#预测新样本，并查看样本的预测结果<br>y_pred &#x3D; clf.predict(Xtest_mms)<br>y_pred<br>(y_pred &#x3D;&#x3D; Ytest).sum() #查看预测正确的样本个数<br>(y_pred &#x3D;&#x3D; Ytest).sum()&#x2F;Ytest.shape[0]  #预测准确率也可以这样算</p><p>五、 KNN算法的优缺点<br>应用广泛<br>此外，最近邻分类属于一类更广泛的技术，这种技术称为基于实例的学习，它使用具体的训练实例进行预测，而不必去维护基于数据建立起来的模型。<br>对样本分类边界不规则的情况较为友好<br>经过长期的实践发现，KNN算法适用于样本分类边界不规则的情况。由于KNN主要依靠周围有限的邻近样本，而不是靠判别类域的方法来确定所属类别，因此对于类域的交叉或重叠较多的待分样本集来说，KNN算法比其他方法要更有效。<br>计算效率低，耗费计算资源较大<br>KNN必须对每一个测试点来计算到每一个训练数据点的距离，并且这些距离点涉及到所有的特征，当数据的维度很大，数据量也很大的时候，KNN的计算会成为诅咒，大概几万数据就足够让KNN跑几个小时了。计算效率低，耗费计算资源较大是KNN最致命的缺点。<br>抗噪性较弱，对噪声数据（异常值）较为敏感<br>最近邻分类器基于局部信息进行预测，正是因为这样的局部分类决策，最近邻分类器（k很小时）对噪声非常敏感。<br>模型不稳定，可重复性较弱<br>最近邻分类器可以生成任意形状的决策边界，这样的决策边界有很高的可变性，因为它们依赖于训练样例的组合。增加最近邻的数目可以降低这种可变性。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习——KNN算法 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
